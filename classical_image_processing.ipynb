{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0012ee",
   "metadata": {},
   "source": [
    "\n",
    "# Image Processing and Feature Detection in Computer Vision\n",
    "\n",
    "## Project Overview\n",
    "This project explores fundamental image processing techniques for **edge detection** and **corner detection**—key concepts in computer vision. These techniques are widely used in applications such as object recognition, autonomous navigation, and medical imaging.\n",
    "\n",
    "The project implements:\n",
    "- **Edge Detection**: Using Gaussian smoothing and gradient computation to identify prominent edges.\n",
    "- **Corner Detection**: Identifying key points in an image using second-order derivatives.\n",
    "\n",
    "We use **convolution operations** with kernels to process images and extract meaningful features. The following sections present the implementation details and visualizations of the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14900dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from scipy.signal import convolve2d\n",
    "from skimage import io\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc970a66",
   "metadata": {},
   "source": [
    "## Edge & Corner Detection\n",
    "\n",
    "### Edge Detection\n",
    "\n",
    "In this task, we implement an **edge detection algorithm** to identify prominent edges in an image. This is a crucial step in many computer vision applications, such as object detection and feature extraction.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Smoothing:**  \n",
    "   To reduce noise and prevent false edges, we apply a **9×9 Gaussian filter** with a standard deviation of **$\\sigma = 1.2$** to smooth the image.\n",
    "\n",
    "2. **Gradient Computation:**  \n",
    "   After smoothing, we compute the image gradient in both the **horizontal ($G_x$)** and **vertical ($G_y$)** directions.  \n",
    "   - The **gradient magnitude** is computed as:  \n",
    "     $$\n",
    "     |G| = \\sqrt{G_x^2 + G_y^2}\n",
    "     $$\n",
    "   - The **gradient direction** is given by:  \n",
    "     $$\n",
    "     \\theta = \\tan^{-1} \\left(\\frac{G_y}{G_x}\\right)\n",
    "     $$\n",
    "\n",
    "3. **Visualization:**  \n",
    "   The results are visualized in four stages:\n",
    "   - **Original Image**\n",
    "   - **Smoothed Image**\n",
    "   - **Gradient Magnitude**\n",
    "   - **Gradient Direction**\n",
    "\n",
    "**Dataset:** The input image used for this task is **`geisel.jpeg`**.\n",
    "\n",
    "### Implementation Notes:\n",
    "- The gradient direction can be calculated using `np.arctan2()`, which returns values in the range **[-180°, 180°]**.\n",
    "- The gradient components (**$G_x$** and **$G_y$**) can be computed via convolution using `convolve2d()` from SciPy.\n",
    "- The following Sobel-like kernels are used for computing the gradients:\n",
    "\n",
    "  ![Convolution Kernels for Image Gradients](./figs/convolution_hint.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian2d(filter_size=1, sig=1.0):\n",
    "    \"\"\"\n",
    "    Creates 2D Gaussian kernel with side length `filter_size` and a sigma of `sig`.\n",
    "    Source: https://stackoverflow.com/a/43346070\n",
    "    \"\"\"\n",
    "    ax = np.arange(-filter_size // 2 + 1., filter_size // 2 + 1.)\n",
    "    xx, yy = np.meshgrid(ax, ax)\n",
    "    kernel = np.exp(-0.5 * (np.square(xx) + np.square(yy)) / np.square(sig))\n",
    "    return kernel / np.sum(kernel)\n",
    "\n",
    "def edge_detect(image):\n",
    "    \"\"\"\n",
    "    Perform edge detection on the image.\n",
    "    \"\"\"\n",
    "    smoothed = smooth(image)\n",
    "    g_mag, g_theta = gradient(smoothed)\n",
    "    return smoothed, g_mag, g_theta\n",
    "\n",
    "def smooth(image):\n",
    "    \"\"\"\n",
    "    Smoothes the image by using a 2D Gaussian kernel.\n",
    "    \n",
    "    Args:\n",
    "        image: input image (h,w)\n",
    "\n",
    "    Returns:\n",
    "        smooth_image: smoothed version of input image (h,w)\n",
    "    \"\"\"\n",
    "    smooth_image = np.zeros_like(image)\n",
    "    filter_size = 9 # Kernel size\n",
    "    sig = 1.2 # STD\n",
    "    \n",
    "    ### \n",
    "    gaussian_kernel = gaussian2d(filter_size, sig)\n",
    "    # perform gaussian smoothing using convolution\n",
    "    smooth_image = convolve2d(image, gaussian_kernel)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return smooth_image\n",
    "\n",
    "def gradient(image):\n",
    "    \"\"\"\n",
    "    Computes a gradient direction image and a gradient magnitude image.\n",
    "\n",
    "    Args:\n",
    "        image: input image (h,w)\n",
    "\n",
    "    Returns:\n",
    "        g_mag: gradient magnitude (h,w)\n",
    "        g_theta: gradient direction (h,w)\n",
    "    \"\"\"\n",
    "    g_mag = np.zeros_like(image)\n",
    "    g_theta = np.zeros_like(image)\n",
    "\n",
    "    ### \n",
    "    k_x = 0.5 * np.array([[0,0,0], [1,0,-1],[0,0,0]])\n",
    "    k_y = 0.5 * np.array([[0,1,0], [0,0,0], [0,-1,0]])\n",
    "\n",
    "    df_dx = convolve2d(image, k_x)\n",
    "    df_dy = convolve2d(image, k_y)\n",
    "\n",
    "    g_mag = np.sqrt(df_dx**2 + df_dy**2)\n",
    "    g_theta = np.arctan2(df_dy, df_dx)\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    return g_mag, g_theta\n",
    "\n",
    "# Plotting\n",
    "# Load image in grayscale\n",
    "image = io.imread(\"./imgs/geisel.jpeg\", as_gray=True)\n",
    "smoothed, g_mag, g_theta = edge_detect(image)\n",
    "print('Original:')\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Smoothed:')\n",
    "plt.imshow(smoothed, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Gradient magnitude:')\n",
    "plt.imshow(g_mag, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Gradient direction:')\n",
    "plt.imshow(g_theta, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b49e8",
   "metadata": {},
   "source": [
    "## Corner Detection\n",
    "\n",
    "In this task, we implement a **corner detection algorithm** to identify corner-like features in an image. Corner detection is widely used in feature matching, object tracking, and 3D reconstruction.\n",
    "\n",
    "### Approach:\n",
    "\n",
    "To detect corners, we analyze the **second-moment matrix** of local image regions and extract the **minor eigenvalue** to identify high-intensity corner-like features.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Compute Image Gradients**:  \n",
    "   - Compute **horizontal ($G_x$)** and **vertical ($G_y$)** gradients using convolution.\n",
    "   - Square and multiply gradients to obtain $G_x^2$, $G_y^2$, and $G_x G_y$.\n",
    "\n",
    "2. **Compute Second-Moment Matrix**:  \n",
    "   - Apply a **Gaussian filter (standard deviation $\\sigma = 2.0$)** to smooth the gradient components.\n",
    "   - Construct the second-moment matrix for each pixel.\n",
    "\n",
    "3. **Eigenvalue Calculation**:  \n",
    "   - Compute the **minor eigenvalue** of the second-moment matrix.\n",
    "   - Identify the **top 100 corners** with the largest minor eigenvalues.\n",
    "\n",
    "4. **Visualization**:  \n",
    "   - Display the detected corners using the `show_corners_result` function.\n",
    "   - Visualize the minor eigenvalue images using `show_eigen_images`.\n",
    "\n",
    "**Dataset:** The input images used in this task are **`im0.png`** and **`im1.png`**, obtained from the [Middlebury Stereo Dataset](https://vision.middlebury.edu/stereo/data/).\n",
    "\n",
    "### Implementation Notes:\n",
    "\n",
    "- Image gradients are computed using **convolution** with Sobel-like kernels:\n",
    "\n",
    "  ![Convolution Kernels for Image Gradients](./figs/convolution_hint.png)\n",
    "\n",
    "- **Corner locations may not align perfectly** with visible corners in the image. This is expected because we detect regions with high minor eigenvalues rather than precise pixel-level corners.\n",
    "- The algorithm may detect **\"corner-like\" features** in areas that mathematically satisfy the derivation, even if they are not visually obvious corners.\n",
    "- When applying a sliding window for filtering, consider **center pixels** in odd-length windows for proper indexing.\n",
    "- **Edge pixels** may contain noise and should be handled carefully when processing the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a4fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_detect(image, n_corners, smooth_std, window_size):\n",
    "    \"\"\"\n",
    "    Detect corners on a given image\n",
    "\n",
    "    Args:\n",
    "        image: 2D grayscale image on which to detect corners\n",
    "        n_corners: Total number of corners to be extracted\n",
    "        smooth_std: Standard deviation of the Gaussian smoothing kernel\n",
    "        window_size: Window size for Gaussian smoothing kernel,\n",
    "                     corner detection, and nonmaximum suppresion\n",
    "\n",
    "    Returns:\n",
    "        minor_eig_image: The minor eigenvalue image (same shape as image)\n",
    "        corners: Detected corners (in x-y coordinates) in a numpy array of shape (n_corners, 2)\n",
    "    \"\"\"\n",
    "    corners = np.zeros((n_corners, 2))\n",
    "\n",
    "    ### \n",
    "    # Step 1. Smooth image with Guassian filter to reduce noise\n",
    "    gaussian_kernel = gaussian2d(window_size, smooth_std)\n",
    "    # perform gaussian smoothing using convolution\n",
    "    image = convolve2d(image, gaussian_kernel, mode=\"same\")\n",
    "    # Step 2. Compute magnitude of x and y gradients at each pixel\n",
    "    k_x = 0.5 * np.array([[0,0,0], [1,0,-1],[0,0,0]])\n",
    "    k_y = 0.5 * np.array([[0,1,0], [0,0,0], [0,-1,0]])\n",
    "\n",
    "    i_x = convolve2d(image, k_x)\n",
    "    i_y = convolve2d(image, k_y)\n",
    "    \n",
    "    # 3. Construct C in a window around each pixel\n",
    "    # define border value for ignoring edge values of image\n",
    "    minor_eig_image = np.zeros_like(image)\n",
    "\n",
    "    border = window_size // 2\n",
    "    for r in range(border, image.shape[0] - border):\n",
    "        for c in range(border, image.shape[1] - border):\n",
    "            # Create matrix C\n",
    "            window_start_r = r - border\n",
    "            window_end_r = r + border\n",
    "            window_start_c = c - border\n",
    "            window_end_c = c + border  # inclusive\n",
    "    \n",
    "            # Ensure window indices are within bounds\n",
    "            window_start_r = max(0, window_start_r)\n",
    "            window_end_r = min(image.shape[0] - 1, window_end_r)\n",
    "            window_start_c = max(0, window_start_c)\n",
    "            window_end_c = min(image.shape[1] - 1, window_end_c)\n",
    "    \n",
    "            # Extract windowed slice from i_x and i_y\n",
    "            i_x_slice = i_x[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "            i_y_slice = i_x[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "\n",
    "            C = np.zeros((2,2))\n",
    "            C[0][0] = np.sum(i_x_slice**2) # sum of Ix^2\n",
    "            C[0][1] = np.sum(i_x_slice*i_y_slice) # sum of Ix*Iy\n",
    "            C[1][0] = np.sum(i_x_slice*i_y_slice)\n",
    "            C[1][1] = np.sum(i_y_slice**2) # sum of Iy^2\n",
    "\n",
    "            # Compute eigenvalues\n",
    "            eigenvalues = np.linalg.eigvals(C)\n",
    "            \n",
    "            # To calcualte min eigenvalue we take abs value\n",
    "            minor_eigenvalue = np.min(np.abs(eigenvalues))\n",
    "\n",
    "            minor_eig_image[r][c] = minor_eigenvalue\n",
    "\n",
    "    suppressed_eig = np.zeros_like(minor_eig_image)\n",
    "    # Apply nonmaximal suppression to minor_eig_image, move window by window size\n",
    "    # Remove \n",
    "    for r in range(border + window_size, image.shape[0] - border, window_size):\n",
    "        for c in range(border + window_size, image.shape[1] - border, window_size):\n",
    "            # Create window\n",
    "            window_start_r = r - border\n",
    "            window_end_r = r + border\n",
    "            window_start_c = c - border\n",
    "            window_end_c = c + border  # inclusive\n",
    "            \n",
    "            # Ensure window indices are within bounds\n",
    "            window_start_r = max(0, window_start_r)\n",
    "            window_end_r = min(image.shape[0] - 1, window_end_r)\n",
    "            window_start_c = max(0, window_start_c)\n",
    "            window_end_c = min(image.shape[1] - 1, window_end_c)\n",
    "            \n",
    "            # Extract windowed slice and choose maximum\n",
    "            slice = minor_eig_image[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "            local_max = np.max(slice)\n",
    "            max_index_window = np.unravel_index(np.argmax(slice), slice.shape)\n",
    "            # Convert from index in window to index in array\n",
    "            max_index_r, max_index_c = window_start_r + max_index_window[0], window_start_c + max_index_window[1]\n",
    "\n",
    "            # Write to the suppressed matrix if the current candidate is the maximum eigenvalue within the grid cell\n",
    "            suppressed_eig[max_index_r][max_index_c] = local_max\n",
    "                \n",
    "    # Find top n points in suppressed_eig\n",
    "    top_corner_indexes = np.argsort(suppressed_eig, axis=None)\n",
    "    sorted_indices = np.argsort(suppressed_eig, axis=None)[-n_corners:]\n",
    "    # unflatten sorted indicies\n",
    "    unraveled = np.unravel_index(sorted_indices, suppressed_eig.shape)\n",
    "    # stack 2-tuple of y and x indicies\n",
    "    corners = np.column_stack((unraveled[1], unraveled[0]))\n",
    "    # print(corners.shape)\n",
    "\n",
    "    # # get indicies of unflattened suprresed_eig\n",
    "    # top_corner_indexes = np.unravel_index(top_corner_indexes, suppressed_eig.shape)\n",
    "    # print(top_corner_indexes.shape)\n",
    "    ### END YOUR CODE\n",
    "    return minor_eig_image, corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_eigen_images(imgs):\n",
    "    print(\"Minor Eigen value images\")\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    # Plot image 1\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(imgs[0], cmap='gray')\n",
    "    plt.title('Image 0')\n",
    "\n",
    "    # Plot image 2\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(imgs[1], cmap='gray')\n",
    "    plt.title('Image 1')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def show_corners_result(imgs, corners, window_size):\n",
    "    print(\"Detected Corners\")\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    ax1 = fig.add_subplot(221)\n",
    "    ax1.imshow(imgs[0], cmap='gray')\n",
    "    ax1.scatter(corners[0][:, 0], corners[0][:, 1], s=25, edgecolors='r', facecolors='none')\n",
    "    ax1.title.set_text(\"Image 0\")\n",
    "\n",
    "    ax2 = fig.add_subplot(222)\n",
    "    ax2.imshow(imgs[1], cmap='gray')\n",
    "    ax2.scatter(corners[1][:, 0], corners[1][:, 1], s=25, edgecolors='r', facecolors='none')\n",
    "    ax2.title.set_text(\"Image 1\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bea830",
   "metadata": {},
   "source": [
    "### You may want to modify these parameters here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect corners on the two provided images\n",
    "\n",
    "# You may want to change these parameters accordingly\n",
    "n_corners = 100\n",
    "smooth_std = 2.0\n",
    "window_size = 13\n",
    "\n",
    "# Read images and detect corners on images\n",
    "imgs = []\n",
    "eig_imgs = []\n",
    "corners = []\n",
    "for i in range(2):\n",
    "    img = io.imread(f\"./imgs/im{i}.png\", as_gray=True)\n",
    "    imgs.append(img)\n",
    "    #img = io.imread('im' + str(i) + '.png')\n",
    "    minor_eig_image, corners_vals = corner_detect(imgs[-1], n_corners, smooth_std, window_size)\n",
    "    eig_imgs.append(minor_eig_image)\n",
    "    corners.append(corners_vals)\n",
    "\n",
    "# Show the results\n",
    "# This may take a few seconds to run\n",
    "show_eigen_images(eig_imgs)\n",
    "show_corners_result(imgs, corners, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612af016",
   "metadata": {},
   "source": [
    "## Task 2: Theory and Solving Example Problems\n",
    "Consider two cameras whose image planes are the z=2 plane, and whose focal  are at (-2, 0, 0) and (8, 0, 0). See Fig 1.1 below. We'll call a point in the first camera (x, y), and a point in the second camera (u, v). Points in each camera are relative to the camera center. So, for example if (x, y) = (0, 0), this is really the point (-2, 0, 2) in world coordinates, while if (u, v) = (0, 0) this is the point (8, 0, 2).\n",
    "\n",
    "Suppose the point $(x,y)=(3,3)$ is matched to the point $(u,v)=(1,3)$. What is the 3D location of this point?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1113cef",
   "metadata": {},
   "source": [
    "From the positions of the focal : $C_1 = (-2, 0, 0), C_2 = (8, 0, 0)$. \n",
    "\n",
    "Next we convert the coordinates (x,y) into world cordinates $x_1^w  = (3+ (-2) ,3+0,2) = (1,3,2)$ and the coordinates (u,v) is  $x_2^w  = (1+8,3+0,2) = (9,3,2)$\n",
    "\n",
    "\n",
    "The direction vector which  in the direction of backprojecting a raw from the image plane to the focal point is:\n",
    "- $x_1^w - C_1 = (1-(-2),3-0,2-0) = (3,3,2)$\n",
    "- $x_2^w - C_2 = (9-(8),3-0,2-0) = (1,3,2)$\n",
    "\n",
    "Therefore the backprojected rays can be defined as:\n",
    "- $R_1(t) = C_1 + t * (3,3,2)$\n",
    "- $R_2(s) = C_2 + s * (1,3,2)$\n",
    "\n",
    "To find the point of intersection:\n",
    "- $R_1(t) = R_2(s)$ \n",
    "- $(-2, 0, 0) + (3t, 3t, 2t)  = (8, 0, 0) + (1s,3s,2s)$\n",
    "\n",
    "Using system of equations:\n",
    "- $-2+3t = 8 + 1s$\n",
    "- $3t =  3s$\n",
    "- $2t = 2s$Ω\n",
    "\n",
    "Thus $t = s$:\n",
    "- $-2 + 3t = 8 + 1t$\n",
    "- $2t = 10$\n",
    "- $t = 5$\n",
    "\n",
    "Substituting t into $R_1$ we get the 3d point:\n",
    "- $(-2, 0, 0) + 5 * (3,3,2) = (-2 + 15, 0 + 15, 0 + 10) = (13, 15, 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5add2ab",
   "metadata": {},
   "source": [
    "### The Epipolar Constraint\n",
    "\n",
    "Suppose two cameras fixate on a point $P$ in space such that their principal axes intersect at that point. (See the fig. 1.2 below.) Show that if the image coordinates are normalized so that the coordinate origin (0, 0) coincides with the principal point, then the $F_{33}$ element of the fundamental matrix is zero.\n",
    "\n",
    "![fig 2.2](./figs/ec_diagram.png)\n",
    "\n",
    "In the figure, $C1$ and $C2$ are the optical centers. The principal axes intersect at point $P$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a340b00",
   "metadata": {},
   "source": [
    "By definition: $x'^T F x = 0$\n",
    "Next we convert the  on the camera planes into homogenous coordinates:\n",
    "- $x = (u,v,1)^T = (0,0,1)$\n",
    "- $x' = (u',v',1)^T = (0,0,1)$\n",
    "\n",
    "Expanding the $x'^T F x = 0$ equation into a linear equation we get:\n",
    "\n",
    "\n",
    "- $uu' f_{11} + vu' f_{12} + u' f_{13} + uv' f_{21} + vv' f_{22} + v' f_{23} + uf_{31} + vf_{32} + f_{33}  = 0$\n",
    "\n",
    "- $0 + 0 + 0 + 0 + 0 + 0 + 0 + 0 + f_{33} = 0$\n",
    "- Therefore $ f_{33} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025ce571",
   "metadata": {},
   "source": [
    "### Essential Matrix\n",
    "\n",
    "Suppose a stereo rig is formed of two cameras: the rotation matrix and translation vector are given to you. Please write down the essential matrix. Also, compute the rank of the essential matrix using SVD, i.e., the number of nonzero singular values. (Note that if you get a singular value $s$ of a very small number in your calculation, e.g., $s<=1e-15$, you can treat it as zero singular value). \n",
    "\n",
    "\n",
    "$$ R=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2} & 0 \\\\\n",
    "-\\frac{1}{2} & \\frac{\\sqrt{3}}{2} & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$ t=\n",
    "\\begin{bmatrix}\n",
    "4 \\\\ 2 \\\\3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c299f71",
   "metadata": {},
   "source": [
    "First we turn translation vector into a 3x3 matrix so that we can take the cross product of $t$ and $p$ through matrix multiplication: $ [t]_x = \n",
    "\\begin{pmatrix}\n",
    "0 & -3 & 2 \\\\\n",
    "3 & 0 & -4 \\\\\n",
    "-2 & 4 & 0 \\\\\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Next we multiply $[t]_X$ with $R$:\n",
    "$\\begin{bmatrix}\n",
    "0 & -3 & 2 \\\\\n",
    "3 & 0 & -4 \\\\\n",
    "-2 & 4 & 0 \\\\\n",
    "\\end{bmatrix} * \\begin{bmatrix}\n",
    "\\frac{\\sqrt{3}}{2} & \\frac{1}{2} & 0 \\\\\n",
    "-\\frac{1}{2} & \\frac{\\sqrt{3}}{2} & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix} =\\begin{bmatrix}\n",
    "\\frac{3}{2} & \\frac{-3\\sqrt{3}}{2} & 2 \\\\\n",
    "-\\frac{3\\sqrt{3}}{2} & \\frac{3}{2} & -4 \\\\\n",
    "-\\sqrt{3} - 2 & 2\\sqrt{3}-1 & 0\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845807e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding rank using SVD\n",
    "t_x = np.array([[0, -3, 2],\n",
    "              [3, 0, -4],\n",
    "              [-2, 4, 0]])\n",
    "\n",
    "R = np.array([[np.sqrt(3)/2, 1/2, 0],\n",
    "              [-1/2, np.sqrt(3)/2, 0],\n",
    "              [0, 0, 1]])\n",
    "\n",
    "# Perform matrix multiplication\n",
    "E = np.dot(t_x, R)\n",
    "\n",
    "# Perform SVD\n",
    "U, S, V = np.linalg.svd(E)\n",
    "\n",
    "# Count number of singular values in S\n",
    "rank = np.sum(S > 0.0000001)\n",
    "\n",
    "\n",
    "\n",
    "print(rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693e9d9",
   "metadata": {},
   "source": [
    "Therefore rank is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45aa151",
   "metadata": {},
   "source": [
    "## Window-Based Similarity Matching: SSD & NCC\n",
    "\n",
    "In this task, we implement **window-based similarity matching** techniques to compare image patches. These methods are commonly used in **stereo vision, template matching, and optical flow estimation**.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "1. **SSD (Sum Squared Distance) Matching**:  \n",
    "   The Sum Squared Distance (SSD) metric computes the difference between two image patches by summing the squared differences of corresponding pixel values. A lower SSD value indicates a better match.\n",
    "\n",
    "   **Formula:**\n",
    "   $$\n",
    "   SSD = \\sum_{x,y} |W_1(x,y) - W_2(x,y)|^2\n",
    "   $$\n",
    "\n",
    "2. **NCC (Normalized Cross-Correlation) Matching**:  \n",
    "   The Normalized Cross-Correlation (NCC) metric measures the similarity between two windows by normalizing their pixel intensities and computing a correlation score. A higher NCC value (closer to **1**) indicates a stronger match.\n",
    "\n",
    "   **Formula:**\n",
    "   $$\n",
    "   NCC = \\frac{\\sum_{x,y} (W_1(x,y) - \\bar{W}_1)(W_2(x,y) - \\bar{W}_2)}\n",
    "   {\\sqrt{\\sum_{x,y} (W_1(x,y) - \\bar{W}_1)^2} \\cdot \\sqrt{\\sum_{x,y} (W_2(x,y) - \\bar{W}_2)^2}}\n",
    "   $$\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "- **`ssd_match` Function**:  \n",
    "  Computes the **Sum Squared Distance (SSD)** for two given image windows.\n",
    "\n",
    "- **`ncc_match` Function**:  \n",
    "  Computes the **Normalized Cross-Correlation (NCC)** for two given image windows.\n",
    "\n",
    "### Applications:\n",
    "- **Stereo Matching**: Identifying corresponding points in left-right image pairs.\n",
    "- **Template Matching**: Locating an object in an image based on similarity.\n",
    "- **Feature Tracking**: Matching keypoints across frames in video sequences.\n",
    "\n",
    "### Implementation Notes:\n",
    "- SSD is **sensitive to intensity differences** and does not handle illumination changes well.\n",
    "- NCC is **scale-invariant** and better suited for cases where intensity variations exist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23873c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_match(img1, img2, c1, c2, R):\n",
    "    \"\"\"\n",
    "    Compute SSD of two windows.\n",
    "    \n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        c1: Center (in x-y coordinates) of the window in image 1.\n",
    "        c2: Center (in x-y coordinates) of the window in image 2.\n",
    "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
    "\n",
    "    Returns:\n",
    "        SSD matching score for two input windows (a scalar value).\n",
    "    \"\"\"\n",
    "    ### \n",
    "    # 1. Find sum of window in image 1\n",
    "    window_start_r = c1[1] - R\n",
    "    window_end_r = c1[1] + R\n",
    "    window_start_c = c1[0] - R\n",
    "    window_end_c = c1[0] + R\n",
    "\n",
    "    window_1 = img1[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "\n",
    "    # # 2. Find sum of window in image 2\n",
    "    window_start_r = c2[1] - R\n",
    "    window_end_r = c2[1] + R\n",
    "    window_start_c = c2[0] - R\n",
    "    window_end_c = c2[0] + R\n",
    "    window_2 = img2[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "\n",
    "    # # 3. Calculate and return matching score\n",
    "    matching_score = np.sum(abs(window_1 - window_2) ** 2)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    return matching_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c36770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code for you to test your implementation\n",
    "img1 = np.array([[1, 2, 3, 4], [4, 5, 6, 8], [7, 8, 9, 4]])\n",
    "img2 = np.array([[1, 2, 1, 3], [6, 5, 4, 4], [9, 8, 7, 3]])\n",
    "\n",
    "result1 = ssd_match(img1, img2, np.array([1, 1]), np.array([1, 1]), 1)\n",
    "# should print 20\n",
    "assert(result1 == 20)\n",
    "print(f\"Result 1: {result1}\")\n",
    "\n",
    "result2 = ssd_match(img1, img2, np.array([2, 1]), np.array([2, 1]), 1)\n",
    "# should print 30\n",
    "assert(result2 == 30)\n",
    "print(f\"Result 2: {result2}\")\n",
    "\n",
    "result3 = ssd_match(img1, img2, np.array([1, 1]), np.array([2, 1]), 1)\n",
    "# should print 46\n",
    "assert(result3 == 46)\n",
    "print(f\"Result 3: {result3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97672cea",
   "metadata": {},
   "source": [
    "## Window-Based Similarity Matching: SSD & NCC\n",
    "\n",
    "In this task, we implement **window-based similarity matching** techniques to compare image patches. These methods are commonly used in **stereo vision, template matching, and optical flow estimation**.\n",
    "\n",
    "### Methods:\n",
    "\n",
    "1. **SSD (Sum Squared Distance) Matching**:  \n",
    "   The Sum Squared Distance (SSD) metric computes the difference between two image patches by summing the squared differences of corresponding pixel values. A lower SSD value indicates a better match.\n",
    "\n",
    "   **Formula:**\n",
    "   $$\n",
    "   SSD = \\sum_{x,y} |W_1(x,y) - W_2(x,y)|^2\n",
    "   $$\n",
    "\n",
    "2. **NCC (Normalized Cross-Correlation) Matching**:  \n",
    "   The Normalized Cross-Correlation (NCC) metric measures the similarity between two windows by normalizing their pixel intensities and computing a correlation score. A higher NCC value (closer to **1**) indicates a stronger match.\n",
    "\n",
    "   **Formula:**\n",
    "   $$\n",
    "   NCC = \\frac{\\sum_{x,y} (W_1(x,y) - \\bar{W}_1)(W_2(x,y) - \\bar{W}_2)}\n",
    "   {\\sqrt{\\sum_{x,y} (W_1(x,y) - \\bar{W}_1)^2} \\cdot \\sqrt{\\sum_{x,y} (W_2(x,y) - \\bar{W}_2)^2}}\n",
    "   $$\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "- **`ssd_match` Function**:  \n",
    "  Computes the **Sum Squared Distance (SSD)** for two given image windows.\n",
    "\n",
    "- **`ncc_match` Function**:  \n",
    "  Computes the **Normalized Cross-Correlation (NCC)** for two given image windows.\n",
    "\n",
    "### Applications:\n",
    "- **Stereo Matching**: Identifying corresponding points in left-right image pairs.\n",
    "- **Template Matching**: Locating an object in an image based on similarity.\n",
    "- **Feature Tracking**: Matching keypoints across frames in video sequences.\n",
    "\n",
    "### Implementation Notes:\n",
    "- SSD is **sensitive to intensity differences** and does not handle illumination changes well.\n",
    "- NCC is **scale-invariant** and better suited for cases where intensity variations exist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebefbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncc_match(img1, img2, c1, c2, R):\n",
    "    \"\"\"\n",
    "    Compute NCC given two windows.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        c1: Center (in image coordinate) of the window in image 1.\n",
    "        c2: Center (in image coordinate) of the window in image 2.\n",
    "        R: R is the radius of the patch, 2 * R + 1 is the window size\n",
    "\n",
    "    Returns:\n",
    "        NCC matching score for two input windows.\n",
    "\n",
    "    \"\"\"\n",
    "    ### \n",
    "    # Calculate W~1\n",
    "    window_start_r = c1[1] - R\n",
    "    window_end_r = c1[1] + R\n",
    "    window_start_c = c1[0] - R\n",
    "    window_end_c = c1[0] + R\n",
    "\n",
    "    window_1 = img1[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "    W_bar = np.mean(window_1)\n",
    "    numerator = window_1 - W_bar\n",
    "    denominator = (window_1 - W_bar) ** 2\n",
    "    denominator = np.sum(denominator)\n",
    "    denominator = np.sqrt(denominator)\n",
    "    W_1 = numerator / denominator\n",
    "\n",
    "    # Calculate W~2\n",
    "    window_start_r = c2[1] - R\n",
    "    window_end_r = c2[1] + R\n",
    "    window_start_c = c2[0] - R\n",
    "    window_end_c = c2[0] + R\n",
    "\n",
    "    window_2 = img2[window_start_r:window_end_r + 1, window_start_c:window_end_c + 1]\n",
    "    W_bar = np.mean(window_2)\n",
    "    numerator = window_2 - W_bar\n",
    "    denominator = (window_2 - W_bar) ** 2\n",
    "    denominator = np.sum(denominator)\n",
    "    denominator = np.sqrt(denominator)\n",
    "    W_2 = numerator / denominator\n",
    "    \n",
    "\n",
    "    ## Multiply W~1 and W~2 and sum resulting matrix\n",
    "    matching_score = np.sum(W_1 * W_2)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return matching_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df00ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code for you to test your implementation\n",
    "img1 = np.array([[1, 2, 3, 4], [4, 5, 6, 8], [7, 8, 9, 4]])\n",
    "img2 = np.array([[1, 2, 1, 3], [6, 5, 4, 4], [9, 8, 7, 3]])\n",
    "\n",
    "result4 = ncc_match(img1, img2, np.array([1, 1]), np.array([1, 1]), 1)\n",
    "# should print 0.8546\n",
    "print(f\"Result 4: {result4}\")\n",
    "\n",
    "result5 = ncc_match(img1, img2, np.array([2, 1]), np.array([2, 1]), 1)\n",
    "# should print 0.8457\n",
    "print(f\"Result 5: {result5}\")\n",
    "\n",
    "result6 = ncc_match(img1, img2, np.array([1, 1]), np.array([2, 1]), 1)\n",
    "# should print 0.6258\n",
    "print(f\"Result 6: {result6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a900fafd",
   "metadata": {},
   "source": [
    "## Feature Correspondence: Naive Matching\n",
    "\n",
    "With the detected corners and the **Normalized Cross-Correlation (NCC) matching function**, we now aim to establish correspondences between features in two images. \n",
    "\n",
    "### Approach:\n",
    "\n",
    "A **naive matching strategy** is used to identify the best correspondence between two sets of detected corners:\n",
    "1. **For each corner in Image 1**, compute the **NCC match score** with every detected corner in Image 2.\n",
    "2. **Select the best match** based on the highest NCC score.\n",
    "3. **Apply a threshold**: If the highest NCC score is below a certain value, no match is assigned.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "- **`naive_matching` Function**:  \n",
    "  - Iterates through detected corners in Image 1.\n",
    "  - Finds the **best match** in Image 2 using the **NCC metric**.\n",
    "  - Returns a set of **matched pairs**.\n",
    "\n",
    "- The function is called as follows:\n",
    "  ```python\n",
    "  matches = naive_matching(corners_img1, corners_img2, image1, image2, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ea244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matching(img1, img2, corners1, corners2, R, NCCth):\n",
    "    \"\"\"\n",
    "    Compute NCC given two windows.\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        corners1: Corners in image 1 (nx2)\n",
    "        corners2: Corners in image 2 (nx2)\n",
    "        R: NCC matching radius\n",
    "        NCCth: NCC matching score threshold\n",
    "\n",
    "    Returns:\n",
    "        matching: NCC matching returns a list of tuple (c1, c2), \n",
    "                  c1 is the 1x2 corner location in image 1, \n",
    "                  c2 is the 1x2 corner location in image 2. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    matching = []\n",
    "    \n",
    "    ### \n",
    "    # 1. For each corner1 in img1, we compute the NCC between it and every corner2 in img2 and save the best match\n",
    "    for c1 in corners1:\n",
    "        # define max and pair for c1\n",
    "        max_match = ncc_match(img1, img2, c1, corners2[0], R)\n",
    "        correspondence_pair = (c1, corners2[0])\n",
    "        for c2 in corners2:\n",
    "            current_ncc = ncc_match(img1, img2, c1, c2, R)\n",
    "            if current_ncc > max_match:\n",
    "                max_match = current_ncc\n",
    "                correspondence_pair =  (c1, c2)\n",
    "        # add corrispondence pair between highest c1 and c2 to matching by stacking if max_match exeeds the threshold\n",
    "        if max_match >= NCCth:\n",
    "            matching.append(correspondence_pair)\n",
    "            # print(correspondence_pair)\n",
    "            \n",
    "    # print(matching)\n",
    "\n",
    "    return matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bda2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    \"\"\" \n",
    "    Convert rgb image to grayscale.\n",
    "    \"\"\"\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "# Detect corners on warrior and matrix sets\n",
    "# You are free to modify the parameters if you wish\n",
    "n_corners = 20\n",
    "smooth_std = 1\n",
    "window_size = 19\n",
    "\n",
    "# Read images and detect corners on the images\n",
    "imgs_mat = []\n",
    "crns_mat = []\n",
    "imgs_war = []\n",
    "crns_war = []\n",
    "\n",
    "for i in range(2):\n",
    "    img_mat = io.imread('./imgs/p4/matrix/matrix' + str(i) + '.png')\n",
    "    imgs_mat.append(rgb2gray(img_mat))\n",
    "    img_war = io.imread('./imgs/p4/warrior/warrior' + str(i) + '.png')\n",
    "    imgs_war.append(rgb2gray(img_war))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef67fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match corners (May need to modify the threshold below)\n",
    "crnsmatf=open('./imgs/p3/crns_mat.pkl','rb')\n",
    "crns_mat=pickle.load(crnsmatf)\n",
    "crnswarf=open('./imgs/p3/crns_war.pkl','rb')\n",
    "crns_war=pickle.load(crnswarf)\n",
    "R = 120\n",
    "NCCth = 0.5  # MAY NEED TO MODIFY YOUR THRESHOLD HERE\n",
    "matching_mat = naive_matching(imgs_mat[0]/255, imgs_mat[1]/255, crns_mat[0], crns_mat[1], R, NCCth)\n",
    "matching_war = naive_matching(imgs_war[0]/255, imgs_war[1]/255, crns_war[0], crns_war[1], R, NCCth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b846c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot matching result\n",
    "def show_matching_result(img1, img2, matching):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.hstack((img1, img2)), cmap='gray') # two dino images are of different sizes, resize one before use\n",
    "    for p1, p2 in matching:\n",
    "        plt.scatter(p1[0], p1[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.scatter(p2[0] + img1.shape[1], p2[1], s=35, edgecolors='r', facecolors='none')\n",
    "        plt.plot([p1[0], p2[0] + img1.shape[1]], [p1[1], p2[1]])\n",
    "    #plt.savefig('matching.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Number of Corners:\", n_corners)\n",
    "show_matching_result(imgs_mat[0], imgs_mat[1], matching_mat)\n",
    "show_matching_result(imgs_war[0], imgs_war[1], matching_war)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc4f764",
   "metadata": {},
   "source": [
    "## Epipolar Geometry & Fundamental Matrix Estimation\n",
    "\n",
    "This section explores **epipolar geometry**, a fundamental concept in **stereo vision** that defines geometric constraints between two images taken from different viewpoints. Instead of relying on brute-force matching, **epipolar constraints** allow us to efficiently find corresponding points along predefined lines, significantly reducing computational complexity.\n",
    "\n",
    "### Understanding Epipolar Geometry:\n",
    "- **Epipolar Constraint**: Corresponding points between two images must lie on **epipolar lines**.\n",
    "- **Fundamental Matrix ($F$)**: A **$3 \\times 3$ matrix** that relates corresponding points between two views.\n",
    "\n",
    "By estimating **$F$**, we can:\n",
    "- **Visualize epipolar lines** for better understanding of correspondences.\n",
    "- **Improve feature matching algorithms** by reducing search space.\n",
    "- **Rectify images** for stereo vision applications (beyond this implementation).\n",
    "\n",
    "---\n",
    "\n",
    "### Computing the Fundamental Matrix (8-Point Algorithm)\n",
    "\n",
    "We use the **8-point algorithm** to estimate the fundamental matrix **$F$** from corresponding points.\n",
    "\n",
    "1. **Construct the constraint matrix ($A$)**:  \n",
    "   - Given **8 or more corresponding points**, we formulate a system of linear equations **$Af = 0$**.\n",
    "  \n",
    "2. **Solve using Singular Value Decomposition (SVD)**:  \n",
    "   - The solution **$f$** corresponds to the **smallest singular vector** of **$A$**, obtained via **SVD**.\n",
    "   - Note: Numpy’s `np.linalg.svd()` returns **$V^T$**, so we use the last row of **$V^T$**.\n",
    "\n",
    "3. **Enforce rank-2 constraint**:  \n",
    "   - The fundamental matrix **$F$** should have rank 2 (handled in the provided code).\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "- **`compute_fundamental` Function**:\n",
    "  - Implements the **8-point algorithm** to estimate **$F$**.\n",
    "  - Uses **SVD** to solve **$Af = 0$**.\n",
    "\n",
    "- **Visualization**:\n",
    "  - **Epipolar lines** are drawn on both images.\n",
    "  - The results help us understand **how points correspond between two views**.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Applications:\n",
    "- **Stereo Matching**: Used to **rectify images** for depth estimation.\n",
    "- **3D Reconstruction**: Helps recover **scene structure** from multiple views.\n",
    "- **Camera Calibration**: Essential in **multi-camera systems**.\n",
    "\n",
    "### Implementation Notes:\n",
    "- **Ensure that point correspondences are normalized** before computing **$F$**.\n",
    "- **Epipolar lines should be plotted** to validate the correctness of the fundamental matrix.\n",
    "- **Improved methods (beyond 8-point)** include the **7-point and RANSAC-based approaches** for handling noise.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b81090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = np.array([[1, 6, 9],\n",
    "              [9, 6, 6],\n",
    "              [8, 4, 1]])\n",
    "\n",
    "# constrain F\n",
    "# make rank 2 by zeroing out last singular value\n",
    "U,S,V = np.linalg.svd(F)\n",
    "S[2] = 0\n",
    "F_prime = np.dot(U,np.dot(np.diag(S),V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import null_space\n",
    "\n",
    "# Find g so F'g = 0 by using null space\n",
    "g = null_space(F_prime)\n",
    "# Invert F_prime for h\n",
    "h = null_space(F_prime.T)\n",
    "\n",
    "print(\"g\",g)\n",
    "print(\"h\",h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745be65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fundamental(x1, x2):\n",
    "    \"\"\"\n",
    "    Computes the fundamental matrix from corresponding points \n",
    "    (x1,x2 3xn arrays) using the 8 point algorithm.\n",
    "        \n",
    "    Construct the A matrix according to lecture\n",
    "    and solve the system of equations for the entries of the fundamental matrix.\n",
    "\n",
    "    Args:\n",
    "        x1: Point correspondences from img1 (3xn)\n",
    "        x2: Point correspondences from img2 (3xn)\n",
    "    \n",
    "    Returns:\n",
    "        Fundamental Matrix (3x3)\n",
    "    \"\"\"\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "    \n",
    "    ### \n",
    "    # Build cols of A\n",
    "    u = x1[0, :] # col 7\n",
    "    u_prime = x2[0, :] # col 3\n",
    "    v = x1[1, :].T # col 8\n",
    "    v_prime = x2[1, :] # col 6\n",
    "\n",
    "    uu_prime = u * u_prime # col 1,\n",
    "    vu_prime = v * u_prime # col 2\n",
    "    uv_prime = u * v_prime # col 4\n",
    "    vv_prime = v * v_prime # col 5\n",
    "    ones = np.ones_like(vv_prime) # col 9\n",
    "    # print(v.shape)\n",
    "    \n",
    "    # Build A using the corrispondences (n x 9), concatenate rows top to bottom then transpose\n",
    "    A = np.vstack((uu_prime, vu_prime, u_prime, uv_prime, vv_prime, v_prime, u, v, ones))\n",
    "    A = A.T\n",
    "\n",
    "    # Solve system of equations using svd\n",
    "    U,S,Vt = np.linalg.svd(A)\n",
    "\n",
    "    # Extract fundemental matrix from last column of V\n",
    "    F = np.reshape(Vt.T[:, -1], (3,3))\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    # constrain F\n",
    "    # make rank 2 by zeroing out last singular value\n",
    "    U,S,V = np.linalg.svd(F)\n",
    "    S[2] = 0\n",
    "    F = np.dot(U,np.dot(np.diag(S),V))\n",
    "    \n",
    "    return F/F[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72e651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental_matrix(x1,x2):\n",
    "    # Normalization of the corner points is handled here\n",
    "    n = x1.shape[1]\n",
    "    if x2.shape[1] != n:\n",
    "        raise ValueError(\"Number of points don't match.\")\n",
    "\n",
    "    # normalize image coordinates\n",
    "    x1 = x1 / x1[2]\n",
    "    mean_1 = np.mean(x1[:2],axis=1)\n",
    "    S1 = np.sqrt(2) / np.std(x1[:2])\n",
    "    T1 = np.array([[S1,0,-S1*mean_1[0]],[0,S1,-S1*mean_1[1]],[0,0,1]])\n",
    "    x1 = np.dot(T1,x1)\n",
    "    \n",
    "    x2 = x2 / x2[2]\n",
    "    mean_2 = np.mean(x2[:2],axis=1)\n",
    "    S2 = np.sqrt(2) / np.std(x2[:2])\n",
    "    T2 = np.array([[S2,0,-S2*mean_2[0]],[0,S2,-S2*mean_2[1]],[0,0,1]])\n",
    "    x2 = np.dot(T2,x2)\n",
    "\n",
    "    # compute F with the normalized coordinates\n",
    "    F = compute_fundamental(x1,x2)\n",
    "\n",
    "    # reverse normalization\n",
    "    F = np.dot(T2.T,np.dot(F,T1))\n",
    "\n",
    "    return F/F[2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fe9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the code for you to test your implementation\n",
    "cor1 = np.load(\"./imgs/p4/\"+'dino'+\"/cor1.npy\")\n",
    "cor2 = np.load(\"./imgs/p4/\"+'dino'+\"/cor2.npy\")\n",
    "print(fundamental_matrix(cor1,cor2))\n",
    "# Should print \n",
    "#[[ 4.00502510e-07 -2.69900666e-06  1.37819769e-03]\n",
    "# [ 3.09619039e-06 -1.00972419e-08 -7.29675791e-03]\n",
    "# [-2.86966053e-03  6.70452915e-03  1.00000000e+00]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "tuvalu = np.array([[1, 9, 0, 1]])\n",
    "vectors = np.array([\n",
    "    [0, 9, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [1, 50, 0, 0],\n",
    "    [1, 6, 0, 1],\n",
    "    [1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "cosine_similarities = cosine_similarity(tuvalu, vectors).flatten()\n",
    "\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da43fc6",
   "metadata": {},
   "source": [
    "## Visualizing Epipolar Geometry\n",
    "\n",
    "Once the **fundamental matrix ($F$)** is computed, we can **visualize epipolar geometry** by plotting **epipolar lines** on both images of a stereo pair. This helps confirm the correctness of our **$F$** estimation.\n",
    "\n",
    "### Purpose:\n",
    "- **Validate the computed fundamental matrix** by checking if corresponding points lie on their respective epipolar lines.\n",
    "- **Gain insights into stereo geometry** by observing how points are constrained along epipolar lines.\n",
    "- **Ensure accurate correspondences** before using epipolar constraints for further tasks like rectification.\n",
    "\n",
    "### Visualization:\n",
    "\n",
    "- We use the function **`plot_epipolar_lines`** to draw the epipolar lines on both images.\n",
    "- Run the provided cells and analyze the results.\n",
    "\n",
    "### Expected Output:\n",
    "- Epipolar lines should **pass through corresponding points** in both images.\n",
    "- The visualized **epipolar lines for \"matrix\" and \"warrior\"** should resemble the example figures below.\n",
    "\n",
    "  ![Dino Epipolar 1](./figs/dinoEpi1.png)\n",
    "  ![Dino Epipolar 2](./figs/dinoEpi2.png)\n",
    "\n",
    "### Notes:\n",
    "- **If the epipolar lines do not align correctly**, revisit the **fundamental matrix computation** in `compute_fundamental`.\n",
    "- **Outliers in feature matching** can cause misalignment—this is common in real-world datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357179e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epipolar_lines(img1, img2, cor1, cor2):\n",
    "    \"\"\"\n",
    "    Plot epipolar lines on image given image, corners\n",
    "\n",
    "    Args:\n",
    "        img1: Image 1.\n",
    "        img2: Image 2.\n",
    "        cor1: Corners in homogeneous image coordinate in image 1 (3xn)\n",
    "        cor2: Corners in homogeneous image coordinate in image 2 (3xn)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    assert cor1.shape[0] == 3\n",
    "    assert cor2.shape[0] == 3\n",
    "    assert cor1.shape == cor2.shape\n",
    "    \n",
    "    F = fundamental_matrix(cor1, cor2)\n",
    "        \n",
    "    # epipole in image 1 is the solution to Fe = 0\n",
    "    U,S,V = np.linalg.svd(F)\n",
    "    e1 = V[-1]\n",
    "    e1 /= e1[-1]\n",
    "    \n",
    "    # epipole in image 2 is the solution to F.Te = 0\n",
    "    U,S,V = np.linalg.svd(F.T)\n",
    "    e2 = V[-1]\n",
    "    e2 /= e2[-1]\n",
    "\n",
    "    plot_epipoles = False\n",
    "    \n",
    "    # Plot epipolar lines in the first image\n",
    "    # There is an epipolar line for each corner\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    h, w = img1.shape[:2]\n",
    "    for c2 in cor2.T:\n",
    "        # epipolar line is (F.T * c2) dot (x, y, 1) = 0\n",
    "        epi_line = np.dot(F.T, c2)\n",
    "        a, b, c = epi_line # ax + by + c = 0, y = -a/b * x - c/b\n",
    "        x = np.arange(w)\n",
    "        y = (-a / b) * x - (c / b)\n",
    "        x = np.array([x[i] for i in range(x.size) if y[i] >=0 and y[i] < h - 1])\n",
    "        y = np.array([y[i] for i in range(y.size) if y[i] >=0 and y[i] < h - 1])\n",
    "        plt.plot(x, y, 'b', zorder=1)\n",
    "        \n",
    "    plt.scatter(cor1[0], cor1[1], s=50, edgecolors='b', facecolors='r', zorder=2)\n",
    "    \n",
    "    if plot_epipoles:\n",
    "        plt.scatter([e1[0]], [e1[1]], s=75, edgecolors='g', facecolors='y', zorder=3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot epipolar lines in the second image\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    h, w = img2.shape[:2]\n",
    "    \n",
    "    for c1 in cor1.T:\n",
    "        # epipolar line is (F * c1) dot (x, y, 1) = 0\n",
    "        epi_line = np.dot(F, c1)\n",
    "        a, b, c = epi_line\n",
    "        x = np.arange(w)\n",
    "        y = (-a / b) * x - (c / b)\n",
    "        x = np.array([x[i] for i in range(x.size) if y[i] >=0 and y[i] < h - 1])\n",
    "        y = np.array([y[i] for i in range(y.size) if y[i] >=0 and y[i] < h - 1])\n",
    "        plt.plot(x, y, 'b', zorder=1)\n",
    "    \n",
    "    plt.scatter(cor2[0], cor2[1], s=50, edgecolors='b', facecolors='r', zorder=2)\n",
    "    \n",
    "    if plot_epipoles:\n",
    "        plt.scatter([e2[0]], [e2[1]], s=75, edgecolors='g', facecolors='y', zorder=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace images and corners with those of matrix and warrior\n",
    "imgids = [\"dino\", \"matrix\", \"warrior\"]\n",
    "for imgid in imgids:\n",
    "    I1 = io.imread(\"./imgs/p4/\"+imgid+\"/\"+imgid+\"0.png\")\n",
    "    I2 = io.imread(\"./imgs/p4/\"+imgid+\"/\"+imgid+\"1.png\")\n",
    "    cor1 = np.load(\"./imgs/p4/\"+imgid+\"/cor1.npy\")\n",
    "    cor2 = np.load(\"./imgs/p4/\"+imgid+\"/cor2.npy\")\n",
    "    plot_epipolar_lines(I1,I2,cor1,cor2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1237d0",
   "metadata": {},
   "source": [
    "## Conclusions and Insights\n",
    "\n",
    "This project successfully explored various computational methods and algorithmic solutions. The key takeaways include:\n",
    "\n",
    "- The importance of optimizing algorithms for efficiency and scalability.\n",
    "- How different data structures impact computational performance.\n",
    "- The significance of visualization in interpreting computational results.\n",
    "\n",
    "### Future Enhancements:\n",
    "- Applying these techniques to real-world datasets for validation.\n",
    "- Exploring parallel computing techniques for further optimization.\n",
    "- Expanding the study to include machine learning models and predictive analytics.\n",
    "\n",
    "This project serves as a foundation for further exploration in computational sciences and software engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42d996",
   "metadata": {},
   "source": [
    "\n",
    "## Key Takeaways\n",
    "\n",
    "Through this project, we successfully implemented:\n",
    "- **Edge detection** using Gaussian smoothing and gradient computation.\n",
    "- **Feature detection** through convolution and filtering techniques.\n",
    "- **Image processing techniques** essential in computer vision.\n",
    "\n",
    "### Future Enhancements:\n",
    "- Implement Canny edge detection for better results.\n",
    "- Extend feature detection to work with real-world images.\n",
    "- Experiment with different kernel sizes and parameters.\n",
    "\n",
    "This project provides a solid foundation for deeper exploration into **computer vision** and **machine learning**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
